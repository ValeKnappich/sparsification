{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069420a7-d67d-4225-b45a-555852b039ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e3153c-f8d5-472d-a95e-efb9b13f26d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Reusing dataset sst (/home/studio-lab-user/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7afdcb9447d4c7baaf3c3c31ca19562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-43f5bb448e8129c6.arrow\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-eead062aa69beb34.arrow\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-0ef065857ea93057.arrow\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint and data\n",
    "\n",
    "dm = src.datamodules.SSTDataModule(32, \"roberta-large\")\n",
    "model = src.models.SSTModel.load_from_checkpoint(\"../logs/runs/2022-01-02/14-46-39/checkpoints/epoch_000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb9ac5f4-dcc0-4841-af28-2da2505accba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:24<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run prediction and extract the labels and predictions of examples where the binary classification is wrong\n",
    "\n",
    "import tqdm\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for batch in tqdm.tqdm(dm.test_dataloader()):\n",
    "    for batch in tqdm.tqdm(dm.val_dataloader()):\n",
    "\n",
    "        for field in batch:\n",
    "            batch[field] = batch[field].cuda()\n",
    "\n",
    "        out = model(batch)\n",
    "        mask = torch.where(out > .5, 1, 0) != torch.where(batch[\"label\"] > .5, 1, 0)\n",
    "        data.extend([(pred.item(), label.item()) for pred, label in zip(out[mask], batch[\"label\"][mask])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f421d9-c2f4-4df2-b9e6-7f17350a731b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.histogram(\n",
       "hist=tensor([ 1.,  4., 20., 42., 62., 72., 13.,  2.,  3.,  1.]),\n",
       "bin_edges=tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
       "        0.9000, 1.0000]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Histogram of labels of the wrong examples\n",
    "# --> model is mostly just wrong on examples close to 0.5\n",
    "\n",
    "torch.tensor([x[1] for x in data]).histogram(10, range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fc7376-1bfa-4c13-93ad-14a25125e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = dm.dataset_dict[\"train\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7659d1f-3460-4157-ae72-19f66839bf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_labels == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e4569dd-d4af-4ca2-8716-21feb673d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = src.models.SSTModel.load_from_checkpoint(\"../logs/runs/2022-01-07/08-36-52/checkpoints/epoch_000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35902b83-86d9-42b5-b8df-cae5b6d45117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:02<00:00, 14.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.histogram(\n",
       "hist=tensor([ 0.,  3., 10., 24., 33., 50., 24., 25.,  6.,  2.]),\n",
       "bin_edges=tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
       "        0.9000, 1.0000]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run prediction and extract the labels and predictions of examples where the binary classification is wrong\n",
    "\n",
    "import tqdm\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for batch in tqdm.tqdm(dm.test_dataloader()):\n",
    "    for batch in tqdm.tqdm(dm.val_dataloader()):\n",
    "\n",
    "        for field in batch:\n",
    "            batch[field] = batch[field].cuda()\n",
    "\n",
    "        out = model(batch)\n",
    "        mask = torch.where(out > .5, 1, 0) != torch.where(batch[\"label\"] > .5, 1, 0)\n",
    "        data.extend([(pred.item(), label.item()) for pred, label in zip(out[mask], batch[\"label\"][mask])])\n",
    "        \n",
    "torch.tensor([x[1] for x in data]).histogram(10, range=(0,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
